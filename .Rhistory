is.bipartite(g3)
plot (g3,layout=layout.bipartite,
vertex.color=c("green","cyan")[V(g3)$type+1])
mbm <- maximum.bipartite.matching(g3)
sum(E(g3)$weight)
library(igraph)
N <- 5
g3 <- graph.full.bipartite (N,N)
#Name the vertices A1...AN and B1..BN
V(g3)$name <- c(paste0("A", 1:N), paste0("B", 1:N))
#set the edge weights
set.seed(122)
E(g3)$weight <- sample(10,N^2, replace=T) #use your fWgt function here instead
#verifty if we did things right
str(g3, TRUE)
is.bipartite(g3)
plot (g3,layout=layout.bipartite,
vertex.color=c("green","cyan")[V(g3)$type+1])
mbm <- maximum.bipartite.matching(g3)
sum(E(g3)$weight)
g <- graph.formula( a-b-c-d-e-f )
m1 <- c("b", "a", "d", "c", "f", "e")   # maximal matching
m2 <- c("b", "a", "d", "c", NA, NA)     # non-maximal matching
m3 <- c("b", "c", "d", "c", NA, NA)     # not a matching
is.matching(g, m1)
is.matching(g, m2)
is.matching(g, m3)
is.maximal.matching(g, m1)
is.maximal.matching(g, m2)
is.maximal.matching(g, m3)
library(igraph)
g <- graph.formula( a-b-c-d-e-f )
m1 <- c("b", "a", "d", "c", "f", "e")   # maximal matching
m2 <- c("b", "a", "d", "c", NA, NA)     # non-maximal matching
m3 <- c("b", "c", "d", "c", NA, NA)     # not a matching
is.matching(g, m2)
V(g)$type <- c(FALSE,TRUE)
str(g, v=TRUE)
maximum.bipartite.matching(g)
str(g, v=TRUE)
maximum.bipartite.matching(g)
addProviderTiles("Stamen.Toner", group = "Toner") %>%
library(leaflet)
#df <- read.csv("PRELIM.csv", header=TRUE)
#df <- read.csv("latlong.csv", header=TRUE)
#df <- na.exclude(df)
#df<-df[-2,]
#head(df$long)
#head(df$lat)
twiticon  <- makeIcon(
iconUrl = "http://leafletjs.com/docs/images/leaf-green.png",
iconWidth = 38, iconHeight = 95,
iconAnchorX = 22, iconAnchorY = 94
)
pal <- colorFactor(c("navy", "red"), domain = c(0, 1))
m <- leaflet()   %>% setView(lng = -1.24757, lat = 50.862676, zoom = 9)%>%
#addTiles() %>%  # Add default OpenStreetMap map tiles  set PO15 5RR as default view
addTiles(group = "OSM (default)") %>%
addProviderTiles("Stamen.Toner", group = "Toner") %>%
addProviderTiles("Stamen.TonerLite", group = "Toner Lite") %>%
addProviderTiles("Stamen.Watercolor", group = "Stamen Watercolor") %>%
addProviderTiles("CartoDB.Positron", group = "CartoDB.Positron") %>%
addProviderTiles("CartoDB.DarkMatter", group = "CartoDB.DarkMatter") %>%
# addMarkers( data = df, lat = ~ lat, lng = ~ long , popup=~username ,icon=twiticon, clusterOptions= markerClusterOptions()) %>%
# Layers control
addLayersControl(
baseGroups = c("OSM (default)", "Toner", "Toner Lite","Stamen Watercolor","CartoDB.Positron", "CartoDB.DarkMatter"),
overlayGroups = c("df", "Outline"),
options = layersControlOptions(collapsed = FALSE)
)
library(leaflet)
install.packages("leaflet")
library(leaflet)
twiticon  <- makeIcon(
iconUrl = "http://leafletjs.com/docs/images/leaf-green.png",
iconWidth = 38, iconHeight = 95,
iconAnchorX = 22, iconAnchorY = 94
)
pal <- colorFactor(c("navy", "red"), domain = c(0, 1))
m <- leaflet()   %>% setView(lng = -1.24757, lat = 50.862676, zoom = 9)%>%
#addTiles() %>%  # Add default OpenStreetMap map tiles  set PO15 5RR as default view
addTiles(group = "OSM (default)") %>%
addProviderTiles("Stamen.Toner", group = "Toner") %>%
addProviderTiles("Stamen.TonerLite", group = "Toner Lite") %>%
addProviderTiles("Stamen.Watercolor", group = "Stamen Watercolor") %>%
addProviderTiles("CartoDB.Positron", group = "CartoDB.Positron") %>%
addProviderTiles("CartoDB.DarkMatter", group = "CartoDB.DarkMatter") %>%
# addMarkers( data = df, lat = ~ lat, lng = ~ long , popup=~username ,icon=twiticon, clusterOptions= markerClusterOptions()) %>%
# Layers control
addLayersControl(
baseGroups = c("OSM (default)", "Toner", "Toner Lite","Stamen Watercolor","CartoDB.Positron", "CartoDB.DarkMatter"),
overlayGroups = c("df", "Outline"),
options = layersControlOptions(collapsed = FALSE)
)
m
lines(predict(xy.lm), y, col='blue')
set.seed(2)
x <- 1:100
y <- 20 + 3 * x
e <- rnorm(100, 0, 60)
y <- 20 + 3 * x + e
plot(x,y)
yx.lm <- lm(y ~ x)
lines(x, predict(yx.lm), col='red')
xy.lm <- lm(x ~ y)
lines(predict(xy.lm), y, col='blue')
xyNorm <- cbind(x=x-mean(x), y=y-mean(y))
plot(xyNorm)
#covariance
xyCov <- cov(xyNorm)
eigenValues <- eigen(xyCov)$values
eigenVectors <- eigen(xyCov)$vectors
plot(xyNorm, ylim=c(-200,200), xlim=c(-200,200))
lines(xyNorm[x], eigenVectors[2,1]/eigenVectors[1,1] * xyNorm[x])
lines(xyNorm[x], eigenVectors[2,2]/eigenVectors[1,2] * xyNorm[x])
View(edgelist)
View(eigenVectors)
plot(xy)
lines(x, (eigenVectors[2,1]/eigenVectors[1,1] * xyNorm[x]) + mean(y))
plot(xy)
lines(x, (eigenVectors[2,1]/eigenVectors[1,1] * xyNorm[x]) + mean(y))
# that looks right. line through the middle as expected
# what if we bring back our other two regressions?
lines(x, predict(yx.lm), col='red')
lines(predict(xy.lm), y, col='blue')
Prior1<-function(n){
mu1<-NULL
mu2<-NULL
lower<-NULL
upper<-NULL
tau<-NULL
for (i in 1:n){
mu1[i]<-runif(1,0,1)
mu2[i]<-runif(1,0,1)
lower[i]<-max(0,(1/mu1[i])+(1/mu2[i])-(1/(mu1[i]*mu2[i])))
upper[i]<-min((1/mu1[i]),(1/mu2[i]))
tau[i]<-runif(1,lower[i],upper[i])
}
return(data.frame(mu1,mu2,tau,lower,upper))
}
Prior1(10)
?igraph_famous
library(igraph)
?igraph_famous
?igraph_ring
library(networkD3)
# load data into a matrix
data <- read.csv(file='Assets-Liabilities.csv', skip=1)
rownames(data) <- data[,1]
data <- as.matrix(data[,-(1:2)])
library(parallel)
no_cores <- detectCores() - 1
no_cores
shiny::runApp('Dropbox/Rcode/LDAShiny')
ecb = function(x,y){ plot(x,t='n'); text(x,labels=iris$Species, col=colors[iris$Species]) }
names(colors) = unique(iris$Species)
ecb = function(x,y){ plot(x,t='n'); text(x,labels=iris$Species, col=colors[iris$Species]) }
tsne_iris = tsne(iris[,1:4], epoch_callback = ecb, perplexity=50)
# compare to PCA
install.packages("tsna")
install.packages("tsne")
library(tsne)
## Not run:
colors = rainbow(length(unique(iris$Species)))
names(colors) = unique(iris$Species)
ecb = function(x,y){ plot(x,t='n'); text(x,labels=iris$Species, col=colors[iris$Species]) }
tsne_iris = tsne(iris[,1:4], epoch_callback = ecb, perplexity=50)
# compare to PCA
dev.new()
pca_iris = princomp(iris[,1:4])$scores[,1:2]
plot(x,t='n')
text(pca_iris, labels=iris$Species,col=colors[iris$Species])
shiny::runApp('Dropbox/Rcode/jobvacs/jobvacancies')
runApp('Dropbox/Rcode/jobvacs/jobvacancies')
runApp('Dropbox/Rcode/jobvacs/jobvacancies')
runApp('Dropbox/Rcode/jobvacs/jobvacancies')
runApp('Dropbox/Rcode/jobvacs/jobvacancies')
runApp('Dropbox/Rcode/jobvacs/jobvacancies')
runApp('Dropbox/Rcode/jobvacs/jobvacancies')
runApp('Dropbox/Rcode/jobvacs/jobvacancies')
runApp('Dropbox/Rcode/jobvacs/jobvacancies')
runApp('Dropbox/Rcode/jobvacs/jobvacancies')
runApp('Dropbox/Rcode/jobvacs/jobvacancies')
runApp('Dropbox/Rcode/jobvacs/jobvacancies')
runApp('Dropbox/Rcode/jobvacs/jobvacancies')
runApp('Dropbox/Rcode/jobvacs/jobvacancies')
runApp('Dropbox/Rcode/jobvacs/jobvacancies')
runApp('Dropbox/Rcode/jobvacs/jobvacancies')
runApp('Dropbox/Rcode/jobvacs/jobvacancies')
runApp('Dropbox/Rcode/jobvacs/jobvacancies')
runApp('Dropbox/Rcode/jobvacs/jobvacancies')
runApp('Dropbox/Rcode/jobvacs/jobvacancies')
runApp('Dropbox/Rcode/jobvacs/jobvacancies')
library(plyr)
library(ggplot2)
library(sp)
library(maptools)
#library(threejs)
library(reshape2)
library(rgdal)
library(rgeos)
library(ggmap)
install.packages("rgdal")
library(rgdal)
library(plyr)
library(ggplot2)
library(sp)
library(maptools)
#library(threejs)
library(reshape2)
library(rgdal)
library(rgeos)
library(ggmap)
install.packages("ggmap")
library(plyr)
library(ggplot2)
library(sp)
library(maptools)
#library(threejs)
library(reshape2)
library(rgdal)
install.packages(c('rzmq','repr','IRkernel','IRdisplay'),
repos = c('http://irkernel.github.io/', getOption('repos')))
install.packages(c('rzmq','repr','IRkernel','IRdisplay'),
repos = c('http://irkernel.github.io/', getOption('repos')))
install.packages(c('rzmq'),
repos = c('http://irkernel.github.io/', getOption('repos')))
install.packages(c('IRKernel'),
repos = c('http://irkernel.github.io/', getOption('repos')))
install.packages(c('IRkernel'),
repos = c('http://irkernel.github.io/', getOption('repos')))
install.packages(c(repr','IRkernel','IRdisplay'),
repos = c('http://irkernel.github.io/', getOption('repos')),
type = 'source')
install.packages(c('IRkernel','IRdisplay'),
repos = c('http://irkernel.github.io/', getOption('repos')),
type = 'source')
library(rvest)
library(dplyr)
library(ggplot2)
install.packages("rvest")
library(rvest)
library(dplyr)
library(ggplot2)
# set up the page we're starting from
page <- html_session('http://www2.autotrader.co.uk/search/used/cars/ford/postcode/so163ty/radius/25/maximum-mileage/up_to_60000_miles/transmission/automatic/onesearchad/used/sort/default/searchcontext/default/page/1')
# set up
cars.all <- NULL
# set up the page we're starting from
page <- html_session("http://www2.autotrader.co.uk/search/used/cars/ford/postcode/so163ty/radius/25/maximum-mileage/up_to_60000_miles/transmission/automatic/onesearchad/used/sort/default/searchcontext/default/page/1")
# set up
cars.all <- NULL
repeat{
# read in the page, extract the results from the search
res <- read_html(page$url) %>%
html_nodes('.search-result__content')
# quick function to extract single node using xpath
column <- function(x) { res %>% html_node(xpath = x) %>% html_text(trim=TRUE) }
# not 100% sure why I'm using node for some and nodes for others. It works though.
cars.page <- data.frame(
title = res %>% html_nodes(xpath='//*[@class="search-result__title"]') %>% html_text(trim=TRUE),
year = substr(column("ul//li[1]"), 1,4),
type = column("ul//li[2]"),
mileage = column("ul//li[3]"),
gearbox = column("ul//li[4]"),
# engine = column("ul//li[5]"), not all listings have an engine size lposted
fuel = res %>% html_nodes(xpath='//ul//li[(contains(text(),"Diesel") or contains(text(),"Petrol"))]')  %>% html_text(),
price = res %>% html_nodes(xpath='//*[@class="search-result__price"]') %>% html_text(),
stringsAsFactors = FALSE
)
# add the cars from each page to the results df
cars.all = rbind(cars.all, cars.page)
# go through all pages - click the 'next' link and follow it
page <- tryCatch(page %>% follow_link(xpath='//a[@class="pagination--right__active"]'),
error=function(e)(return(NULL)))
# check the header
print(page)
# break out if no more pages
if(is.null(page)) break
# just in case - random break
Sys.sleep(round(runif(1, min=45, max=90)))
}
library(rvest)
library(dplyr)
library(ggplot2)
# set up the page we're starting from
page <- html_session('http://www2.autotrader.co.uk/search/used/cars/ford/postcode/so163ty/radius/25/maximum-mileage/up_to_60000_miles/transmission/automatic/onesearchad/used/sort/default/searchcontext/default/page/1')
# set up
cars.all <- NULL
repeat{
# read in the page, extract the results from the search
res <- read_html(page$url) %>%
html_nodes('.search-result__content')
# quick function to extract single node using xpath
column <- function(x) { res %>% html_node(xpath = x) %>% html_text(trim=TRUE) }
# not 100% sure why I'm using node for some and nodes for others. It works though.
cars.page <- data.frame(
title = res %>% html_nodes(xpath='//*[@class="search-result__title"]') %>% html_text(trim=TRUE),
year = substr(column("ul//li[1]"), 1,4),
type = column("ul//li[2]"),
mileage = column("ul//li[3]"),
gearbox = column("ul//li[4]"),
# engine = column("ul//li[5]"), not all listings have an engine size lposted
fuel = res %>% html_nodes(xpath='//ul//li[(contains(text(),"Diesel") or contains(text(),"Petrol"))]')  %>% html_text(),
price = res %>% html_nodes(xpath='//*[@class="search-result__price"]') %>% html_text(),
stringsAsFactors = FALSE
)
# add the cars from each page to the results df
cars.all = rbind(cars.all, cars.page)
# go through all pages - click the 'next' link and follow it
page <- tryCatch(page %>% follow_link(xpath='//a[@class="pagination--right__active"]'),
error=function(e)(return(NULL)))
# check the header
print(page)
# break out if no more pages
if(is.null(page)) break
# just in case - random break
Sys.sleep(round(runif(1, min=45, max=90)))
}
# do a bit of plotting
library(stringr)
cars.all %>%
mutate(make = tolower(word(title, 1))) %>%
mutate(model = tolower(word(title, 2))) %>%
mutate(mileage = gsub(' miles', '', mileage)) %>%
mutate(mileage = as.numeric(gsub(',', '', mileage))) %>%
mutate(price = gsub('£', '', price)) %>%
mutate(price = as.numeric(gsub(',', '', price)) ) %>%
ggplot(aes(mileage, price, color=model)) +
geom_point() + geom_smooth(method = 'lm', se = FALSE)
res <- read_html(page$url) %>%
html_nodes('.search-result__content')
?read_html
install.packages("xml2")
library(dplyr)
library(xml2)
library(ggplot2)
# set up the page we're starting from
page <- html_session('http://www2.autotrader.co.uk/search/used/cars/ford/postcode/so163ty/radius/25/maximum-mileage/up_to_60000_miles/transmission/automatic/onesearchad/used/sort/default/searchcontext/default/page/1')
# set up
cars.all <- NULL
repeat{
# read in the page, extract the results from the search
res <- read_html(page$url) %>%
html_nodes('.search-result__content')
# quick function to extract single node using xpath
column <- function(x) { res %>% html_node(xpath = x) %>% html_text(trim=TRUE) }
# not 100% sure why I'm using node for some and nodes for others. It works though.
cars.page <- data.frame(
title = res %>% html_nodes(xpath='//*[@class="search-result__title"]') %>% html_text(trim=TRUE),
year = substr(column("ul//li[1]"), 1,4),
type = column("ul//li[2]"),
mileage = column("ul//li[3]"),
gearbox = column("ul//li[4]"),
# engine = column("ul//li[5]"), not all listings have an engine size lposted
fuel = res %>% html_nodes(xpath='//ul//li[(contains(text(),"Diesel") or contains(text(),"Petrol"))]')  %>% html_text(),
price = res %>% html_nodes(xpath='//*[@class="search-result__price"]') %>% html_text(),
stringsAsFactors = FALSE
)
# add the cars from each page to the results df
cars.all = rbind(cars.all, cars.page)
# go through all pages - click the 'next' link and follow it
page <- tryCatch(page %>% follow_link(xpath='//a[@class="pagination--right__active"]'),
error=function(e)(return(NULL)))
# check the header
print(page)
# break out if no more pages
if(is.null(page)) break
# just in case - random break
Sys.sleep(round(runif(1, min=45, max=90)))
}
# do a bit of plotting
library(stringr)
cars.all %>%
mutate(make = tolower(word(title, 1))) %>%
mutate(model = tolower(word(title, 2))) %>%
mutate(mileage = gsub(' miles', '', mileage)) %>%
mutate(mileage = as.numeric(gsub(',', '', mileage))) %>%
mutate(price = gsub('£', '', price)) %>%
mutate(price = as.numeric(gsub(',', '', price)) ) %>%
ggplot(aes(mileage, price, color=model)) +
geom_point() + geom_smooth(method = 'lm', se = FALSE)
rm(list = ls())
# set up the page
library(rvest)
library(dplyr)
library(xml2)
library(ggplot2)
rm(list = ls())
# set up the page we're starting from
page <- html_session('http://www2.autotrader.co.uk/search/used/cars/volkswagen/postcode/so163ty/radius/25/maximum-mileage/up_to_60000_miles/transmission/automatic/onesearchad/used/sort/default/searchcontext/default/page/1')
# set up
cars.all <- NULL
repeat{
# read in the page, extract the results from the search
res <- read_html(page$url) %>%
html_nodes('.search-result__content')
# quick function to extract single node using xpath
column <- function(x) { res %>% html_node(xpath = x) %>% html_text(trim=TRUE) }
# not 100% sure why I'm using node for some and nodes for others. It works though.
cars.page <- data.frame(
title = res %>% html_nodes(xpath='//*[@class="search-result__title"]') %>% html_text(trim=TRUE),
year = substr(column("ul//li[1]"), 1,4),
type = column("ul//li[2]"),
mileage = column("ul//li[3]"),
gearbox = column("ul//li[4]"),
# engine = column("ul//li[5]"), not all listings have an engine size lposted
fuel = res %>% html_nodes(xpath='//ul//li[(contains(text(),"Diesel") or contains(text(),"Petrol"))]')  %>% html_text(),
price = res %>% html_nodes(xpath='//*[@class="search-result__price"]') %>% html_text(),
stringsAsFactors = FALSE
)
# add the cars from each page to the results df
cars.all = rbind(cars.all, cars.page)
# go through all pages - click the 'next' link and follow it
page <- tryCatch(page %>% follow_link(xpath='//a[@class="pagination--right__active"]'),
error=function(e)(return(NULL)))
# check the header
print(page)
# break out if no more pages
if(is.null(page)) break
# just in case - random break
Sys.sleep(round(runif(1, min=45, max=90)))
}
# do a bit of plotting
library(stringr)
cars.all %>%
mutate(make = tolower(word(title, 1))) %>%
mutate(model = tolower(word(title, 2))) %>%
mutate(mileage = gsub(' miles', '', mileage)) %>%
mutate(mileage = as.numeric(gsub(',', '', mileage))) %>%
mutate(price = gsub('£', '', price)) %>%
mutate(price = as.numeric(gsub(',', '', price)) ) %>%
ggplot(aes(mileage, price, color=model)) +
geom_point() + geom_smooth(method = 'lm', se = FALSE)
library(rvest)
library(dplyr)
library(xml2)
library(ggplot2)
rm(list = ls())
# set up the page we're starting from
page <- html_session('http://www2.autotrader.co.uk/search/used/cars/volkswagen/golf/postcode/so163ty/radius/25/page/1/onesearchad/used/maximum-mileage/up_to_60000_miles/searchcontext/default/sort/default')
# set up
cars.all <- NULL
repeat{
# read in the page, extract the results from the search
res <- read_html(page$url) %>%
html_nodes('.search-result__content')
# quick function to extract single node using xpath
column <- function(x) { res %>% html_node(xpath = x) %>% html_text(trim=TRUE) }
# not 100% sure why I'm using node for some and nodes for others. It works though.
cars.page <- data.frame(
title = res %>% html_nodes(xpath='//*[@class="search-result__title"]') %>% html_text(trim=TRUE),
year = substr(column("ul//li[1]"), 1,4),
type = column("ul//li[2]"),
mileage = column("ul//li[3]"),
gearbox = column("ul//li[4]"),
# engine = column("ul//li[5]"), not all listings have an engine size lposted
fuel = res %>% html_nodes(xpath='//ul//li[(contains(text(),"Diesel") or contains(text(),"Petrol"))]')  %>% html_text(),
price = res %>% html_nodes(xpath='//*[@class="search-result__price"]') %>% html_text(),
stringsAsFactors = FALSE
)
# add the cars from each page to the results df
cars.all = rbind(cars.all, cars.page)
# go through all pages - click the 'next' link and follow it
page <- tryCatch(page %>% follow_link(xpath='//a[@class="pagination--right__active"]'),
error=function(e)(return(NULL)))
# check the header
print(page)
# break out if no more pages
if(is.null(page)) break
# just in case - random break
Sys.sleep(round(runif(1, min=45, max=90)))
}
# do a bit of plotting
library(nnet)
library(ggplot2)
library(reshape2)
library(maxent)
library(RTextTools)
library(xgboost)
library(randomForest)
#clean up the r environment.
rm(list = ls())
#@home working directory
setwd("/Users/thorosm2002/Dropbox/Rcode/MLinR")
Afile <- 'DEFdry.csv'
rd <- read.csv2(Afile, sep=",", stringsAsFactors=FALSE,header = TRUE)
rd <- rd[,c("ITEMDESC","CCPNODOT","AMTPAID","STNDQUAN","STNDUNIT")]
rd$WEIGHT <- paste0(rd$STNDQUAN," " , as.character(rd$STNDUNIT))
rd <- na.omit(rd)
keptdata <- rd[! (is.na(rd$ITEMDESC) | rd$WEIGHT=="" |
rd$ITEMDESC==""  | is.na(rd$WEIGHT) |
is.na(as.numeric(rd$CCPNODOT)) )   , ]
keptdata <- head(keptdata,8000)
testdata <-  tail(keptdata,2000)
traindata<- head(keptdata,6000)
dtMatrix <- create_matrix(cbind(keptdata["ITEMDESC"],keptdata["WEIGHT"],keptdata["CCPNODOT"],keptdata["AMTPAID"]),
language="english",removeNumbers=FALSE,stemWords=FALSE,removeSparseTerms = .99999)
# Configure the training data
container <- create_container(dtMatrix, keptdata$WEIGHT, trainSize=1:6000, testSize = 6001:8000, virgin=FALSE)
svmmodel <- train_model(container,"SVM")
dtMatrix
write.svm(svmmodel, svm.file = "svm-classifier.svm", scale.file = "svm-classifier.scale")
#maxentmodel <- train_model(container,"MAXENT")
library(e1071)
write.svm(svmmodel, svm.file = "svm-classifier.svm", scale.file = "svm-classifier.scale")
#maxentmodel <- train_model(container,"MAXENT")
results <- classify_model(container,svmmodel)
#results <- classify_model(container,maxentmodel)
#analytics<- create_analytics(container , results )
#cross validation
SVM_CROSS <- cross_validate(container,4,algorithm="SVM")
